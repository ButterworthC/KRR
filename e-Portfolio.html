<html>
<head>
<title>e-Portfolio for Knowledge Representation and Reasoning course Autumn 2024</title>
<style>
.discussion-box {
  background-color: #f0f0f0;  /* Set the background color */
  padding: 20px;              /* Add padding to create a slight indent */
  border: none;               /* Remove any border */
  margin: 20px 0;             /* Add margin for spacing */
  border-radius: 10px;        /* Optional: add rounded corners */
  box-shadow: none;           /* No shadow, keeps it clean */
}

.discussion-box p {
  margin: 10px 0;             /* Space out paragraphs */
}
</style>
</head>

<body style="background-color: ffffbb; margin-left: 50px; margin-right: 50px;">
<h1>e-Portfolio for Knowledge Representation and Reasoning course Autumn 2024</h1>
<h2>Chris Butterworth</h2>
<!-- <p><a href="about-me.html">About me</a></p> -->
<p><a href="https://butterworthc.github.io/ml-course/about-me.html">About me</a></p>
<p><a href="prep.html">Preparation for Knowledge Representation and Reasoning module</a></p>

<p>GitHub-hosted version: <a href="https://butterworthc.github.io/KRR/e-Portfolio.html">butterworthc.github.io/KRR/e-Portfolio.html</a></p>
<p>Online repository, showing commit descriptions: <a href="https://github.com/ButterworthC/KRR">https://github.com/ButterworthC/KRR</a></p>











<h3>Unit 1: Introduction to Agent-Based Computing</h3>
<p><b>Unit 1 Lecturecast</b></p>
<p>I had already read the first chapter of our textbook (<a href="References.html#wooldridge">Wooldridge, 2009</a>) when I saw this, and noticed that it was
the source for most of the lecturecast material. However, the lecturecast was a more concise and memorable presentation of the material.</p>
<p><a href="Readings.html#unit01readings"><b>Unit 1 Readings</b></a></p>

<h3>Unit 2: Introducing First Order Logic</h3>
<p>As described on my "preparation" page linked to above, I read about FOL in the two books shown, and watched a series of videos on the subject.
The concepts are not very difficult to understand but it can sometimes take a long time to turn a convoluted expression into an English sentence.
The biggest problem is the choice of symbols - some of the operators are in very obscure characters and 
some are not included with Microsoft Word&trade;.</p>
<p><a href="Readings.html#unit02readings"><b>Unit 2 Readings</b></a></p>

<h3>Unit 3: Agent Architectures</h3>
<p><b>Unit 3 Lecturecast</b></p>
<p>This explains that agents consist of software and "architecture", of which it gives definitions from <a href="References.html#maes">Maes (1991)</a> and 
<a href="References.html#kaelbling">Kaelbling (1991)</a>, both of which are examined in the readings for this unit. There are sections on representation, 
reasoning and ontology, and an introduction to Brooks Behavioural Language.</p>
<p><a href="Readings.html#unit03readings"><b>Unit 3 Readings</b></a></p>

<h3>Collaborative Discussion 1: Agent Based Systems</h3>

<div class="discussion-box">
<p><b>Summary Post on Agent Based Systems</b></p>
<p>In the first three units of the "Intelligent Agents" module we were introduced to agents, 
particularly the concept of their autonomy and the way they can communicate with each other and their environment.  
First Order Logic was introduced in the second unit, and the readings for the first two units 
were from our standard texts <a href="References.html#wooldridge">Wooldridge (2009)</a> and <a href="References.html#russell">Russell &amp; Norvig (2022)</a>.</p>
<p>I started my initial post by talking about MIT's seminal book from the nineties, 
"Artificial Life: An Overview" and its forward-thinking discussion of a universe of "artificial life" 
consisting of agents and the systems that can be constructed using large numbers of them (<a href="References.html#langton">Langton, 1995</a>).
I then discussed developments since the nineties, including the far more powerful but less expensive computers available now, 
which have enabled these multi-agent systems to be produced in real life without bankrupting everyone involved.</p>
<p>In my last paragraph I talked about some of the applications of these systems, primarily in the field of economics, 
citing a paper that talked about gaming simulations (<a href="References.html#testfatsion">Testfatsion &amp; Judd, 2006</a>), 
which are often described as Monte Carlo simulations.</p>
<p>I had a response from Gavin Viljoen to my initial post, in which he made some insightful comments on what gives agents flexibility.  
The first two of these were decentralization and modularity, which allow agents to work autonomously and adaptably, 
such that systems can be expanded as needed, and not be brought to a halt by the loss of any given agent 
(this kind of redundancy reminds me of internet routers).  
Their adaptability also enables agents to react to and operate in changing environments, 
and their scalability makes the handling of large quantities of data possible <a href="References.html#viljoen">Viljoen, 2024</a>.</p>
<p>In his response, Gavin cited a couple of introductory papers and also an intriguing one about 
"Assessing the Use of Agent-Based Models for Tobacco Regulation" (<a href="References.html#hammond">Hammond, 2015</a>).
</p>
<p>References have been moved to the <a href="References.html">References</a> page.</p>
</div>

<h3>Unit 4: Hybrid Agent Architectures</h3>
<p><a href="Readings.html#unit04reading"><b>Unit 4 Reading</b></a></p>

<h3>Unit 5: Agent Communication</h3>
<p><b>Unit 5 Lecturecast</b></p>
<p>This goes into the ontologies of agentsa, and their alignment.  It is based on the readings for this unit 
but also goes into knowledge query and manipulation language (KQML) and the knowledge interchange format (KIF).</p>
<p><a href="Readings.html#unit05readings"><b>Unit 5 Readings</b></a></p>

<h3>Unit 6: Working Together</h3>
<p><a href="Readings.html#unit06reading"><b>Unit 6 Reading</b></a></p>

<p><b>Creating Agent Dialogues</b></p>
<p>The following messages use the Knowledge Query and Manipulation Language (KQML) <i>request</i> and <i>inform</i> performatives (<a href="References.html#finin_et_al">Finin et al, 1994</a>).</p>
<p>Knowledge Interchange Format (KIF) keywords such as <i>ask-one</i> and <i>and</i> are listed in <a href="References.html#genesereth">Genesereth (1992)</a>.</p>
<p>
<pre>
<b>Request number of 50 inch TVs in stock</b>
(request
  :sender agentAlice
  :receiver agentBob
  :ontology stock
  :language KIF
  :content
  (ask-one
    (and
      (in-stock ?item-name ?quantity)
      (equal ?item-name (TV :screen-size 50)))
  )
)

<b>Response</b>
(inform
  :sender agentBob
  :receiver agentAlice
  :ontology stock
  :language KIF
  :content
  (and
    (in-stock (TV :screen-size 50) 10)
  )
)

<b>Request number of HDMI ports</b>
(request
  :sender agentAlice
  :receiver agentBob
  :ontology stock
  :language KIF
  :content
  (ask-one
    (and
      (has-property ?item-name ?property ?value)
      (equal ?item-name (TV :screen-size 50))
      (equal ?property HDMI-slots))
  )
)

<b>Response</b>
(inform
  :sender agentBob
  :receiver agentAlice
  :ontology stock
  :language KIF
  :content
  (and
    (has-property (TV :screen-size 50) HDMI-slots 2)
  )
)
</pre>
</p>

<h3>Unit 7: Natural Language Processing (NLP)</h3>
<p><b>Unit 7 Lecturecast</b></p>
<p>The grammar and semantics of spoken/written languages are discussed, and natural language processing introduced as computers' methods of understanding them.
The concepts of syntax, semantics and pragmatics are compared in their natural language and computer contexts.
Problems such as ambiguity, vagueness and polysemy are described, and the role of statistical analysis of word distributions.
Systems like Word2Vec are introduced, with some Python code. 
Finally we get definitions of Hearst Patterns, hyponyms and types of parsing.
</p>
<p><a href="Readings.html#unit07readings"><b>Unit 7 Readings</b></a></p>

<h3>Collaborative Discussion 2: Agent Communication Languages</h3>
Since my initial post received no responses, I reproduce it here instead of a summary:
<div class="discussion-box">
<p><b>Initial Post on Agent Communication Languages</b></p>
<p>A scenario described by <a href="References.html#labrou">Labrou &amp; Finin (1994)</a> is a calendar whose data is accessed 
by various separate systems for different purposes such as scheduling meetings 
(as is commonplace in Microsoft Outlook&trade; 30 years later).
The authors thought this would be a good candidate for the Knowledge Sharing Effort (KSE) treatment, 
which had been described a couple of years earlier (<a href="References.html#patil">Patil et al, 1992</a>).
They related how Knowledge Query Manipulation Language (KQML) arose from this work.</p>
<p>Some advantages of such Agent Communication Languages include richer semantics that could convey 
the meaning of requests and their responses, similar to HTTP but with more meaning.  
Also like HTTP was the distributed nature of the participating nodes and the asynchronicity of their communication.
By adopting a universal message format, the agents themselves would not need to be written in the same language or 
even by people who knew each other, or even worked in the same decade.</p>
<p>The main disadvantage I see is that languages such as KQML offer no more flexibility than 
an even more universal format such as JSON, where just about anything can be represented.</p>
<p>The question of comparison with method invocation in Python or Java is puzzling, 
since methods are invoked only within one assembly.  There is no asynchronicity or distribution involved.
</p>
<p>References have been moved to the <a href="References.html">References</a> page.</p>
</div>

<h3>Unit 8: Understanding Natural Language Processing (NLP)</h3>
<p><a href="Readings.html#unit08reading"><b>Unit 8 Reading</b></a></p>

<p><b>Unit 8 Activity: Creating Parse Trees</b></p>
<p>The activity calls for three phrases to be converted to "constituency-based parse trees" but since these are not mentioned in 
<a href="References.html#zimmerman">Zimmerman (2019)</a> I looked up another source, <a href="References.html#chomsky">Chomsky (1957)</a> which says that these are
the parse trees described by Zimmerman.</p>
<p>Python has a library called Natural Language Toolkit (<a href="References.html#nltk">NLTK, 2024</a>) which enables these parse trees 
to be visualised almost as well as in
<a href="References.html#zimmerman">Zimmerman (2019)</a>, so I entered the three sentences as follows, with two versions of the ambiguous one with the dog,
and the resulting parse trees are shown. First, I imported the Natural Language Toolkit:<br>
<img src="images/import_nltk.jpg" width="230" height="37" /><br clear="all" />
Then I populated the Tree objects with the edges and each word they classify.  The output is obtain via the draw() method of tree: 
<img src="images/TheGovernmentRaisedInterestRates.jpg" width="644" height="273" /><br clear="all" />
<img src="images/TheInternetGivesEveryoneAVoice.jpg" width="651" height="331" /><br clear="all" />
<img src="images/TheManSawTheDog_WithTheTelescope.jpg" width="725" height="426" /><br clear="all" />
<img src="images/TheManSaw_TheDogWithTheTelescope.jpg" width="770" height="425" /><br clear="all" />
My notebook is on: <a href="https://github.com/ButterworthC/IntelligentAgents/blob/main/notebooks/ParseTrees.ipynb">GitHub</a>.
</p>

<h3>Unit 9: Introduction to Adaptive Algorithms</h3>
<p><b>Unit 9 Lecturecast</b></p>
<p>This lecturecast introduced artificial neural networks and discussed their basic structure, including layers for inputs and outputs, 
with hidden layers in between. Feature engineering, training and testing are very briefly mentioned, and then the example of handwriting recognition
is discussed.  Deep learning is described as a process which uses the neural network to engineer features as well as train on them,
and convolutional neural networks are introduced, having matrixes which can reduce the volume of data or recognise features.
We used CNNs to classify images in the Machine Learning module, but here the emphasis is on character recognition.</p>
<p><a href="Readings.html#unit09reading"><b>Unit 9 Reading</b></a></p>

<h3>Unit 10: Deep Learning in Action</h3>
<p><a href="Readings.html#unit10reading"><b>Unit 10 Reading</b></a></p>

<h3>Unit 11: Intelligent Agents in Action</h3>
<p><b>Unit 11 Lecturecast</b></p>
<p>This unit's lecturecast summarises the readings.</p>
<p><a href="Readings.html#unit11readings"><b>Unit 11 Readings</b></a></p>

<h3>Collaborative Discussion 3: Deep Learning</h3>
Since my initial post received no responses, I reproduce it here instead of a summary:
<div class="discussion-box">
<p><b>Initial Post on Deep Learning</b></p>
<p>The depth of deep learning lies in the multiple hidden layers between the input and output layers of its neural networks.
That, and the ability to engineer their own features, is what makes deep learning networks stand out from simpler machine learning models 
(<a href="References.html#brundage">Brundage et al., 2018</a>; <a href="References.html#goodfellow">Goodfellow et al., 2016</a>).</p>
<p>The ethical dimension of artificial intelligence, and in particular generative AI, 
such as ChatGPT&trade; (<a href="References.html#openai">OpenAI, 2024</a>) and other systems 
that can output "original" material such as text, images and video, is coming under scrutiny 
as people begin to worry where the rise of this technology will leave them.  
There are concerns about fake news being disseminated by AI systems which have ingested opinionated posts on social media, 
without adequate guarantees of objectivity.
This is not deliberate bias but the result of unchecked biased inputs echoing around various deep learning systems.
There is also the problem of copyright when information such as writing, images and music are output without acknowledgement of their 
original sources, which can get lost among the many layers of these systems (<a href="References.html#brundage">Brundage et al., 2018</a>).</p>
<p>Countermeasures are being developed, themselves using AI methods of detecting bias in order to clean up data sources.
This process has been called algorithmic hygiene (<a href="References.html#leent">Lee et al., 2019</a>) and it is currently focused on 
the areas of recruitment, facial recognition and online advertising.  
It is to be hoped that deep learning can be given an ethical framework.</p>
<p>References have been moved to the <a href="References.html">References</a> page.</p>
</div>

<h3>Unit 12: The Future of Intelligent Agents</h3>
<p><a href="Readings.html#unit12reading"><b>Unit 12 Reading</b></a></p>


<h3>Individual Project</h3>


<h3>References</h3>
<p><a href="References.html">References.html</a></p>

<p>&nbsp;</p>
<p>&nbsp;</p>

</body>
</html>